# ğŸ§  Latent Surprise in Earnings Calls â€“ preJEPA Prototype

This repository contains the prototype of a pipeline designed to detect **implicit surprise** in earnings call transcripts, by measuring semantic dissociation between the **prepared remarks** (`intro`, `body`) and the **Q&A session**.  
This work is a stepping stone toward a future implementation of a **Joint Embedding Predictive Architecture (JEPA)** for predictive modeling of financial communications.

---

## ğŸ¯ Goal

The goal is to uncover **non-obvious, latent signals** in earnings calls â€” especially in the Q&A â€” that are not captured by traditional quantitative metrics (EPS surprise, guidance) but may explain **market reaction** or highlight **information asymmetry**.

---

## ğŸ› ï¸ Whatâ€™s in this prototype?

âœ”ï¸ Intelligent segmentation of earnings call transcripts (`intro`, `body`, `qna`)  
âœ”ï¸ Zero-shot classification using `facebook/bart-large-mnli`  
âœ”ï¸ Embedding generation with `all-MiniLM-L6-v2`  
âœ”ï¸ Semantic dissociation scoring (cosine distance) between sections  
âœ”ï¸ Structured export and pipeline-ready `.json` / `.npz` files  
âœ”ï¸ Filtering of metadata & short utterances (`meta`)  
âœ”ï¸ Generation of a **latent surprise score** per document

---

## ğŸ”¬ What this is *not* (yet)

This is **not yet a JEPA**, but a **pre-JEPA** phase:
- No prediction of latent space yet (no `g(f(x)) â‰ˆ f(y)`)
- No learned latent filtering or contrastive learning
- No supervised or autoregressive generation

---

## ğŸ“ Folder structure

```text
data/
â”œâ”€â”€ raw/                   # .docx earnings call transcripts
â”œâ”€â”€ annotated_blocks/      # per-paragraph zero-shot classification
â”œâ”€â”€ processed_structured/  # structured JSONs (intro/body/qna)
â”œâ”€â”€ embeddings/            # .npz embeddings
â”œâ”€â”€ results/               # JSONs with surprise scores
src/
â”œâ”€â”€ parse_docx.py
â”œâ”€â”€ zero_shot_parser.py
â”œâ”€â”€ assemble_sections.py
â”œâ”€â”€ generate_embeddings.py
â”œâ”€â”€ compute_surprise.py
```

---

## ğŸ“Š Example output

```json
{
  "basename": "apple_q2_2025",
  "surprise_intro_qna": 0.832,
  "surprise_body_qna": 0.816,
  "surprise_intro_body": 0.787
}
```

## ğŸš€ Roadmap
âœ… Pre-JEPA dissociation scoring (this repo)

ğŸ”œ Trainable JEPA to predict Q&A latent space from intro+body

ğŸ”œ Latent error filtering module (topic-sensitive or contrastive)

ğŸ”œ Correlation with market variables (return, volatility)

ğŸ”œ Extension to generative hallucination: â€œwhat questions should have been asked?â€

## ğŸ“š Citation & academic intent
This project is part of a prospective research thesis (CIFRE) on representation learning and surprise detection in financial communication, using self-supervised predictive architectures inspired by JEPA.
If you're interested in collaboration or supervision, feel free to open an issue or contact me directly.

## ğŸ§‘â€ğŸ’» Author
Benjamin SÃ©guin
HEC MontrÃ©al, Quantitative Finance & AI
benjamin.seguin@hec.ca
